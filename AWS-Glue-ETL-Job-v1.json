{
	"jobConfig": {
		"name": "AWS-Glue-ETL-Job-v1",
		"description": "",
		"role": "arn:aws:iam::807046468650:role/AWS-GLUE-IAM-ROLE",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 2,
		"maxCapacity": 2,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "AWS-Glue-ETL-Job-v1.py",
		"scriptLocation": "s3://aws-glue-s3-bucket/scripts/",
		"language": "python-3",
		"spark": false,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-02-06T07:08:17.111Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-s3-bucket/tmp/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-s3-bucket/tmp/logs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null,
		"pythonPath": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nfrom awsglue.transforms import *\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\nfrom awsglue.dynamicframe import DynamicFrame\r\nfrom pyspark.sql.functions import coalesce,col,when,sum,avg,row_number,count,year,month,dayofmonth,to_date,date_format,to_timestamp,trim,rank,row_number,dense_rank\r\nfrom pyspark.sql.window import Window\r\nfrom datetime import datetime\r\n\r\n## @params: [JOB_NAME]\r\nargs=getResolvedOptions(sys.argv, ['JOB_NAME'])#when running gluejob there are many params passsed ,here in this line of code we are extracting 'JOB_NAME' and storing it in 'args' , \"JOB_NAME\" Identifies the Glue job as a whole.\r\n\r\n#sys.argv --> List of arguments passed when the job runs.\r\n#'JOB_NAME' --> The name of the argument we need to extract.\r\n#'getResolvedOptions()'-->Extracts only the required argument from sys.argv.\r\n#Why is it needed? --> AWS Glue requires a job name to track execution.\r\n\r\nsc = SparkContext() #SparkContext is the entry point for working with Apache Spark. (Tells Spark to start running.) , its like engine starter in a car\r\n\r\n#Allocates resources memory & CPUs for data processing.\r\n#In AWS Glue, it is needed to enable distributed data processing . (Allows interaction with Spark RDDs (Resilient Distributed Datasets))\r\n\r\n\r\nglueContext = GlueContext(sc) #Extend Spark with Glue features\r\n\r\nspark = glueContext.spark_session #Allows to work with data using dataframes and sql.\r\n\r\n\r\njob = Job(glueContext) #This creates a Glue job object to manage the job execution.\r\n\r\n#It tracks the job’s state (start, progress, success, failure)\r\n#It is required when you want to commit job progress in AWS Glue.\r\n\r\n\r\njob.init(args['JOB_NAME'], args) #Initializes the job with a name.\r\n\r\n\r\nglue_dynamic_frame_initial = glueContext.create_dynamic_frame.from_catalog(database='my-aws-glue-db', table_name='ufo_reports_source_csv_table')\r\n\r\n#You want to use DynamicFrame when Data that does not confirm to a fixed schema.\r\n#Data is stored in AWS Glue’s optimized format. AWS Glue’s special version of a DataFrame, optimized for ETL.\r\n#Can be converted to Spark DataFrame\r\n\r\n#what happense if we dont use DynamicFrame ??\r\n#1.) We cannot directly load AWS Glue Catalog tables 2.) Glue’s built-in transformations wont work 3.)Schema evolution and automatic data cleaning won’t be available.\r\n\r\ndf_spark = glue_dynamic_frame_initial.toDF() # DynamicFrame to Spark df\r\n\r\ntimestamp_backup = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\r\ns3_path = f\"s3://aws-glue-s3-bucket/ufo_reports_source_csv_backup/{timestamp_backup}/\"\r\n\r\ndf_spark.coalesce(1).write.mode(\"overwrite\").csv(s3_path, header=True) #taking backup before processing further\r\n\r\n## Rename the columns, extract year and drop unnecessary columns. Remove NULL records\"\"\"\r\n\r\ndef prepare_dataframe(df):\r\n    \"\"\"Rename columns, extract year, drop unnecessary columns, and remove rows containing NULLs or blanks.\"\"\"\r\n    \r\n    # Rename columns for consistency\r\n    df_renamed = df.withColumnRenamed(\"Shape Reported\", \"shape_reported\") \\\r\n        .withColumnRenamed(\"Colors Reported\", \"color_reported\") \\\r\n        .withColumnRenamed(\"State\", \"state\") \\\r\n        .withColumnRenamed(\"Time\", \"time\")\r\n    \r\n    df_renamed = df_renamed.filter(\r\n        (col(\"shape_reported\").isNotNull()) & (trim(col(\"shape_reported\")) != \"\") &\r\n        (col(\"color_reported\").isNotNull()) & (trim(col(\"color_reported\")) != \"\"))\r\n    \r\n    timestamp_col = coalesce(\r\n        to_timestamp(col(\"time\"), \"M/d/yyyy H:mm\"),\r\n        to_timestamp(col(\"time\"), \"MM-dd-yyyy HH:mm\")\r\n    )\r\n\r\n    df_year_added = df_renamed.withColumn(\"year\", year(timestamp_col)).drop(\"city\").drop(\"time\")\r\n\r\n    return df_year_added\r\n\r\n#Create color and shape dataframes and join them\"\r\n\r\ndef join_dataframes(df):\r\n    shape_grouped = df.groupBy(\"year\", \"state\", \"shape_reported\") \\\r\n        .agg(F.count(\"*\").alias(\"shape_occurrence\"))\r\n\r\n    color_grouped = df.groupBy(\"year\", \"state\", \"color_reported\") \\\r\n        .agg(F.count(\"*\").alias(\"color_occurrence\"))\r\n\r\n    df_joined = shape_grouped.join(color_grouped,\r\n                                on=[\"year\", \"state\"],\r\n                                how=\"inner\")\r\n    \r\n    return df_joined\r\n\r\n\r\n#Rank based on colour and shape occurances\r\n\r\ndef create_final_dataframe(df):\r\n    \"\"\"Create final dataframe\"\"\"\r\n    shape_window_spec = Window.partitionBy(\"year\", \"state\").orderBy(col(\"shape_occurrence\").desc()) \r\n    color_window_spec = Window.partitionBy(\"year\", \"state\").orderBy(col(\"color_occurrence\").desc())\r\n\r\n    # Selecting top occurrences of shape and color per year and state\r\n    final_df = df.withColumn(\"shape_rank\", row_number().over(shape_window_spec)) \\\r\n        .withColumn(\"color_rank\", row_number().over(color_window_spec)) \\\r\n        .filter((col(\"shape_rank\") == 1) & (col(\"color_rank\") == 1)) \\\r\n        .select(\"year\", \"state\", \"shape_reported\", \"shape_occurrence\", \"color_reported\", \"color_occurrence\",\"shape_rank\",\"color_rank\") \\\r\n        .orderBy(col(\"shape_occurrence\").desc())\r\n    \r\n    return final_df\r\n\r\ndf_prepared = prepare_dataframe(df_spark)\r\ndf_joined = join_dataframes(df_prepared)\r\ndf_final = create_final_dataframe(df_joined)\r\n\r\n\r\n\r\n# From Spark dataframe to glue dynamic frame\r\nglue_dynamic_frame_final = DynamicFrame.fromDF(df_final, glueContext, \"glue_etl\") #\t\"glue_etl\" Labels and tracks a specific transformation step within the job.\r\n\r\n# Generate timestamped folder\r\ntimestamp_target = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\r\ns3_path = f\"s3://aws-glue-s3-bucket/ufo_reports_target_csv/{timestamp_target}/\"\r\n\r\n# Write DynamicFrame to S3 as CSV\r\nglueContext.write_dynamic_frame.from_options(\r\n    frame=glue_dynamic_frame_final,\r\n    connection_type=\"s3\",\r\n    connection_options={\"path\": s3_path},\r\n    format=\"csv\",\r\n    format_options={\"separator\": \",\", \"quoteChar\": '\"'},\r\n)\r\n\r\n\r\n## job.commit() should be the last line of teh script\r\njob.commit() # Without job.commit(), AWS Glue doesn’t know the job is finished!\r\n\r\n"
}